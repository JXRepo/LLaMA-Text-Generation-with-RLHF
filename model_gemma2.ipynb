{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe696015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 6)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 256000\n",
    "padding_idx = 0\n",
    "hidden_size = 2304 // 4\n",
    "num_hidden_layers = 26 // 4\n",
    "\n",
    "hidden_size, num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a827d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 125, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def repeat_kv(x):\n",
    "    #x -> [4, 4, 125, 256]\n",
    "\n",
    "    #[4, 4, 125, 256] -> [4, 4, 2, 125, 256]\n",
    "    x = x.unsqueeze(2).expand(-1, -1, 2, -1, -1)\n",
    "\n",
    "    #[4, 4, 2, 125, 256] -> [4, 8, 125, 256]\n",
    "    return x.reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3],\n",
    "                     x.shape[4])\n",
    "\n",
    "\n",
    "repeat_kv(torch.randn(4, 4, 125, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9980d33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 125, 256]), torch.Size([4, 4, 125, 256]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    #q -> [4, 8, 125, 256]\n",
    "    #k -> [4, 4, 125, 256]\n",
    "    #cos -> [1, 125, 256]\n",
    "    #sin -> [1, 125, 256]\n",
    "\n",
    "    cos = cos.unsqueeze(1)\n",
    "    sin = sin.unsqueeze(1)\n",
    "\n",
    "    def rotate_half(x):\n",
    "        #从最后一个维度上切分\n",
    "        left = x[..., :x.shape[-1] // 2]\n",
    "        right = x[..., x.shape[-1] // 2:]\n",
    "\n",
    "        #左右交换顺序,右边部分符号取反,重新组合在一起\n",
    "        return torch.cat((-right, left), dim=-1)\n",
    "\n",
    "    #和两个三角函数分别加权求和,在qk中融入位置信息\n",
    "    q = (q * cos) + (rotate_half(q) * sin)\n",
    "    k = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q, k\n",
    "\n",
    "\n",
    "out = apply_rotary_pos_emb(torch.randn(4, 8, 125, 256),\n",
    "                           torch.randn(4, 4, 125, 256),\n",
    "                           torch.randn(1, 125, 256), torch.randn(1, 125, 256))\n",
    "\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21afbb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 125, 256]), torch.Size([1, 125, 256]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2RotaryEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #[128] tensor([1.0000e+00, 9.3057e-01, 8.6596e-01, ..., 1.2409e-04, 1.1548e-04, 1.0746e-04])\n",
    "        inv_freq = 1.0 / (1_0000.0**(torch.arange(0, 256, 2) / 256))\n",
    "\n",
    "        #[128] -> [1, 128, 1]\n",
    "        inv_freq = inv_freq.reshape(1, -1, 1)\n",
    "\n",
    "        self.register_buffer('inv_freq', tensor=inv_freq, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, lens):\n",
    "        position_ids = torch.arange(lens,\n",
    "                                    device=self.inv_freq.device,\n",
    "                                    dtype=self.inv_freq.dtype)\n",
    "        position_ids = position_ids.reshape(1, 1, -1)\n",
    "\n",
    "        #[1, 128, 1] * [1, 1, 125] -> [1, 128, 125]\n",
    "        freqs = self.inv_freq.matmul(position_ids)\n",
    "\n",
    "        #[1, 128, 125] -> [1, 125, 128]\n",
    "        freqs = freqs.transpose(1, 2)\n",
    "\n",
    "        #[1, 125, 128] -> [1, 125, 256]\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        #[1, 125, 256],[1, 125, 256]\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "\n",
    "out = Gemma2RotaryEmbedding()(125)\n",
    "\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f54f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 576])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_proj = torch.nn.Linear(hidden_size, 2048, bias=False)\n",
    "        self.k_proj = torch.nn.Linear(hidden_size, 1024, bias=False)\n",
    "        self.v_proj = torch.nn.Linear(hidden_size, 1024, bias=False)\n",
    "        self.o_proj = torch.nn.Linear(2048, hidden_size, bias=False)\n",
    "        self.rotary_emb = Gemma2RotaryEmbedding()\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        #hidden_states -> [4, 125, hidden_size]\n",
    "        #attention_mask -> [4, 1, 125, 125]\n",
    "\n",
    "        b, lens, _ = hidden_states.size()\n",
    "\n",
    "        #[4, 125, hidden_size] -> [4, 125, 2048]\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        #[4, 125, hidden_size] -> [4, 125, 1024]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        #[4, 125, hidden_size] -> [4, 125, 1024]\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        #[4, 125, 2048] -> [4, 125, 8, 256] -> [4, 8, 125, 256]\n",
    "        query_states = query_states.reshape(b, lens, 8, 256).transpose(1, 2)\n",
    "        #[4, 125, 1024] -> [4, 125, 4, 256] -> [4, 4, 125, 256]\n",
    "        key_states = key_states.reshape(b, lens, 4, 256).transpose(1, 2)\n",
    "        #[4, 125, 1024] -> [4, 125, 4, 256] -> [4, 4, 125, 256]\n",
    "        value_states = value_states.reshape(b, lens, 4, 256).transpose(1, 2)\n",
    "\n",
    "        #[1, 125, 256],[1, 125, 256]\n",
    "        cos, sin = self.rotary_emb(lens)\n",
    "\n",
    "        #维度不变\n",
    "        query_states, key_states = apply_rotary_pos_emb(\n",
    "            query_states, key_states, cos, sin)\n",
    "\n",
    "        #[4, 4, 125, 256] -> [4, 8, 125, 256]\n",
    "        key_states = repeat_kv(key_states)\n",
    "        #[4, 4, 125, 256] -> [4, 8, 125, 256]\n",
    "        value_states = repeat_kv(value_states)\n",
    "\n",
    "        #[4, 8, 125, 256] * [4, 8, 256, 125] -> [4, 8, 125, 125]\n",
    "        atten = query_states.matmul(key_states.transpose(2, 3))\n",
    "        atten = atten * 256**-0.5\n",
    "\n",
    "        #维度不变\n",
    "        atten = (atten / 50.0).tanh() * 50.0\n",
    "        atten = atten + attention_mask\n",
    "\n",
    "        #[4, 8, 125, 125] * [4, 8, 125, 256] -> [4, 8, 125, 256]\n",
    "        atten = atten.softmax(dim=-1).matmul(value_states)\n",
    "\n",
    "        #[4, 8, 125, 256] -> [4, 125, 8, 256] -> [4, 125, 8, 2048]\n",
    "        atten = atten.transpose(1, 2).reshape(b, lens, -1)\n",
    "\n",
    "        #[4, 125, 8, 2048] -> [4, 125, 8, hidden_size]\n",
    "        atten = self.o_proj(atten)\n",
    "\n",
    "        return atten\n",
    "\n",
    "\n",
    "Gemma2Attention()(torch.randn(4, 125, hidden_size),\n",
    "                  torch.randn(4, 1, 125, 125)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305e766a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 576])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_proj = torch.nn.Linear(hidden_size, 9216, bias=False)\n",
    "        self.up_proj = torch.nn.Linear(hidden_size, 9216, bias=False)\n",
    "        self.down_proj = torch.nn.Linear(9216, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [4, 125, hidden_size]\n",
    "\n",
    "        #[4, 125, hidden_size] -> [4, 125, 9216]\n",
    "        left = torch.nn.functional.gelu((self.gate_proj(x)),\n",
    "                                        approximate='tanh')\n",
    "\n",
    "        #[4, 125, hidden_size] -> [4, 125, 9216]\n",
    "        right = self.up_proj(x)\n",
    "\n",
    "        #[4, 125, 9216] -> [4, 125, hidden_size]\n",
    "        return self.down_proj(left * right)\n",
    "\n",
    "\n",
    "Gemma2MLP()(torch.randn(4, 125, hidden_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305f777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 576])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2RMSNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [4, 125, hidden_size]\n",
    "\n",
    "        #约等于二范数的倒数,区别是公式中的mean改成sum\n",
    "        norm2_reciprocal_like = x.pow(2).mean(-1, keepdim=True) + 1e-6\n",
    "        norm2_reciprocal_like = norm2_reciprocal_like.rsqrt()\n",
    "\n",
    "        #约等于除以自己的二范数,起到规范化的作用\n",
    "        x = x * norm2_reciprocal_like\n",
    "\n",
    "        #线性投影\n",
    "        #[4, 125, hidden_size]\n",
    "        x = x * (1.0 + self.weight)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "Gemma2RMSNorm()(torch.randn(4, 125, hidden_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d113eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 576])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2DecoderLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = Gemma2Attention()\n",
    "        self.mlp = Gemma2MLP()\n",
    "        self.input_layernorm = Gemma2RMSNorm()\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm()\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm()\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm()\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        #hidden_states -> [4, 125, hidden_size]\n",
    "        #attention_mask -> [4, 1, 125, 125]\n",
    "\n",
    "        res = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        #维度不变\n",
    "        hidden_states = self.self_attn(hidden_states=hidden_states,\n",
    "                                       attention_mask=attention_mask)\n",
    "\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states) + res\n",
    "\n",
    "        res = hidden_states\n",
    "\n",
    "        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n",
    "\n",
    "        #维度不变\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "\n",
    "        hidden_states = self.post_feedforward_layernorm(hidden_states) + res\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "Gemma2DecoderLayer()(torch.randn(4, 125, hidden_size),\n",
    "                     torch.randn(4, 1, 125, 125)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07abe946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 125, 125])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(attention_mask, dtype, device):\n",
    "    #attention_mask -> [4, 125]\n",
    "\n",
    "    b, lens = attention_mask.shape\n",
    "    min_value = torch.finfo(dtype).min\n",
    "\n",
    "    #填充极大负数\n",
    "    mask = torch.full((lens, lens),\n",
    "                      fill_value=min_value,\n",
    "                      device=device,\n",
    "                      dtype=dtype)\n",
    "\n",
    "    #对角线和对角线以下归零\n",
    "    if lens != 1:\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "    #扩展尺寸\n",
    "    mask = mask.reshape(1, 1, lens, lens)\n",
    "    mask = mask.expand(b, 1, lens, lens)\n",
    "\n",
    "    #pad的位置填充负极大数\n",
    "    pad_mask = attention_mask.reshape(b, 1, 1, lens) == 0\n",
    "    mask = mask.masked_fill(pad_mask, min_value)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "get_mask(torch.randint(0, 2, (4, 125)), torch.float32, 'cpu').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f675e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 576])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = torch.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                               embedding_dim=hidden_size,\n",
    "                                               padding_idx=padding_idx)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [Gemma2DecoderLayer() for _ in range(num_hidden_layers)])\n",
    "        self.norm = Gemma2RMSNorm()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #input_ids -> [4, 125]\n",
    "        #attention_mask -> [4, 125]\n",
    "\n",
    "        #[4, 125] -> [4, 125, hidden_size]\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        #[4, 125] -> [4, 1, 125, 125]\n",
    "        attention_mask = get_mask(attention_mask, hidden_states.dtype,\n",
    "                                  hidden_states.device)\n",
    "\n",
    "        hidden_states = hidden_states * hidden_size**0.5\n",
    "\n",
    "        for layer in self.layers:\n",
    "            #维度不变\n",
    "            hidden_states = layer(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "Gemma2Model()(torch.randint(100, 10000, (4, 125)),\n",
    "              torch.randint(0, 2, (4, 125))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04a0eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12.6092, grad_fn=<NllLossBackward0>), torch.Size([4, 125, 256000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Gemma2ForCausalLM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Gemma2Model()\n",
    "        self.lm_head = torch.nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        #input_ids -> [4, 125]\n",
    "        #attention_mask -> [4, 125]\n",
    "        #labels -> [4, 125]\n",
    "\n",
    "        #[4, 125, hidden_size]\n",
    "        hidden_states = self.model(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask)\n",
    "\n",
    "        #[4, 125, vocab_size]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        logits = (logits / 30.0).tanh() * 30.0\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            #[4, 125, vocab_size] -> [4, 124, vocab_size] -> [4*124, vocab_size]\n",
    "            shift_logits = logits[:, :-1].flatten(end_dim=1)\n",
    "\n",
    "            #[4, 125] -> [4, 124] -> [4*124]\n",
    "            shift_labels = labels[..., 1:].flatten()\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(shift_logits,\n",
    "                                                     shift_labels)\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "out = Gemma2ForCausalLM()(torch.randint(100, 10000, (4, 125)),\n",
    "                          torch.randint(0, 2, (4, 125)),\n",
    "                          torch.randint(100, 10000, (4, 125)))\n",
    "\n",
    "out[0], out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ce446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import Gemma2Config, Gemma2ForCausalLM as Gemma2ForCausalLM_Original\n",
    "\n",
    "# config = \"{'vocab_size': 256000, 'max_position_embeddings': 8192, 'hidden_size': hidden_size, 'intermediate_size': 9216, 'num_hidden_layers': 26, 'num_attention_heads': 8, 'head_dim': 256, 'num_key_value_heads': 4, 'hidden_activation': 'gelu_pytorch_tanh', 'initializer_range': 0.02, 'rms_norm_eps': 1e-06, 'use_cache': True, 'rope_theta': 10000.0, 'attention_bias': False, 'attention_dropout': 0.0, 'attn_logit_softcapping': 50.0, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'bfloat16', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['Gemma2ForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 2, 'pad_token_id': 0, 'eos_token_id': [1, 107], 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'google/gemma-2-2b-it', 'transformers_version': '4.43.3', 'cache_implementation': 'hybrid', 'hidden_act': 'gelu_pytorch_tanh', 'model_type': 'gemma2', 'final_logit_softcapping': 30.0, 'query_pre_attn_scalar': 256, 'sliding_window': 4096}\"\n",
    "# config = Gemma2Config.from_dict(eval(config))\n",
    "# config.vocab_size = vocab_size\n",
    "# config.padding_idx = padding_idx\n",
    "# config.hidden_size = hidden_size\n",
    "# config.num_hidden_layers = num_hidden_layers\n",
    "# config.use_cache = False\n",
    "# config.cache_implementation = None\n",
    "\n",
    "# model = Gemma2ForCausalLM()\n",
    "# model_original = Gemma2ForCausalLM_Original(config)\n",
    "\n",
    "# model.load_state_dict(model_original.state_dict())\n",
    "\n",
    "# input = {\n",
    "#     'input_ids': torch.randint(100, 10000, [4, 125]),\n",
    "#     'attention_mask': torch.ones(4, 125).long(),\n",
    "#     'labels': torch.randint(100, 10000, [4, 125])\n",
    "# }\n",
    "# input['attention_mask'][:, 120:] = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     loss, logits = model(**input)\n",
    "#     out_original = model_original(**input)\n",
    "\n",
    "# loss == out_original.loss, (logits == out_original.logits).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4872be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LlamaModel = Gemma2Model\n",
    "LlamaForCausalLM = Gemma2ForCausalLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
